{
    "attention_probs_dropout_prob": 0.2,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.2,
    "hidden_size": 64,
    "bucket_list": [[0, 63], [64, 751], [752, 4242], [4243, 16800], [16801, 56218], [56219, 175121], [175122, 514653], [514654, 2300000]],
    "factor_list": [64, 41, 27, 17, 11, 7, 5, 3],
    "intermediate_size": 64,
    "initializer_range": 0.02,
    "max_position_embeddings": 200,
    "num_attention_heads": 2,
    "num_hidden_layers": 8,
    "type_vocab_size": 2,
    "vocab_size": 2300000
}